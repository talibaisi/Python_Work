{"cells":[{"metadata":{"_uuid":"357eb9cffa118e57b9851657318f35e292462ee8","_cell_guid":"c5ecbc0b-51a2-45f2-b327-5f2e0e8f5f28"},"cell_type":"markdown","source":"# ** Introduction **\n> ### **About the dataset** : \n>>### Zillow's Economic Research Team collects, cleans and publishes housing and economic data from a variety of public and proprietary sources. Public property record data filed with local municipalities -- including deeds, property facts, parcel information and transactional histories -- form the backbone of our data products and is fleshed out with proprietary data derived from property listings and user behavior on Zillow.\n\n> ### **About this notebook: ** \n>> ### Zillow Economic Dataset has 20 years of housing price data(1996-2017).This sounds like we can predict the price of next years. Predicting the price given the previous price is called time series analysis. So what is time series analysis? According to Wikipedia, A time series is a series of data points indexed in time order. Most commonly, a time series is a sequence taken at successive equally spaced points in time. Thus it is a sequence of discrete-time data.\n\n>> ### And to do time series analysis we will use Long short-term memory network or in short LSTM network. So what is LSTM? An LSTM network is a special kind of neural network that can predict according to the data of previous time. It is popular for language recognition, time series analysis and many more.\n\n>> ### In this notebook, I will be using Keras machine learning library as a Tensorflow backend for implementing LSTM.\n\n\n# ** Table of contents **\n> 1. [Load the data](#load_the_data)\n> 2. [Pre-process the data](#process_data)\n> 3. [Train the model](#train_model)\n> 4. [Predict the house price for next years](#predict_house_price)"},{"metadata":{"_uuid":"4c539f98c6a05874d6cd51bd2cc2a2366abebd4c","_kg_hide-output":true,"_kg_hide-input":true,"_cell_guid":"a3018e36-fdd6-40c3-b50c-66b135bed26e","trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom math import sqrt\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport matplotlib.dates as mdates\nimport seaborn as sns\nimport numpy\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import TimeSeriesSplit\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cb8666d1a33f2fbaf872f80963364c8629c6783e","_cell_guid":"dabfaff8-8415-4e3d-b525-503a9d8d3e38"},"cell_type":"markdown","source":"<a id=\"load_the_data\"></a>\n# ** 1. Load the data **\n> ** For this notebook, I will only use 'City_time_series.csv' file. I will load the file using pandas read_csv method. And keep the mean of ZHVIPerSqft_AllHomes column at same date. Then I will plot the data. **"},{"metadata":{"_uuid":"7cb942e9c1aeefccea6e6ddda68f6c171d24757c","_cell_guid":"9e65c6fe-150e-408c-bdaf-6456250e4e4a","trusted":true},"cell_type":"code","source":"# load the data using pandas build in read csv function\ndf_city_time_series = pd.read_csv('../input/City_time_series.csv',parse_dates=['Date'])\n# drop null values in ZHVIPerSqft_AllHomes because we are interested in this column\ndf_city_time_series = df_city_time_series.dropna(subset=['ZHVIPerSqft_AllHomes'])\n# print the head of our data set\ndf_city_time_series.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"scrolled":false,"_uuid":"67b9172071c4a76130397160dd33508f2eedfd73","_kg_hide-input":true,"_cell_guid":"85515de5-1b32-4ca5-a103-e05baae49b28","trusted":true},"cell_type":"code","source":"# the ZHVIPerSqft_AllHomes column has many value in same date but for different location. \n# For this notebook we are not interested in location. We mean all the value in same date\ndf_zhvi_sqft_all = df_city_time_series.set_index('Date').groupby(pd.Grouper(freq='d')).mean().dropna(how='all').ZHVIPerSqft_AllHomes\ndf_zhvi_sqft_all","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98c35a605224ced1dc3dea60bc65ebeae854db92","_kg_hide-input":true,"_cell_guid":"e0f9008c-70f4-40bd-9641-2b9dbdc0dc09","trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15, 10))\nax.scatter(df_zhvi_sqft_all.index, df_zhvi_sqft_all)\n# change x axis year location interval to 1 year. So that it displays data in interval of 1 year\nax.xaxis.set_major_locator(mdates.YearLocator(1))\n# Add the title to the graph\nplt.title('Zillow Home Value Index in Per Square foot in different year', fontsize=18)\n# add xlabel\nplt.xlabel('Year', fontsize=18)\n# add ylabel\nplt.ylabel('Zillow Home Value Index in Per Square foot', fontsize=18)\n# beautify the x axis date presentation\nfig.autofmt_xdate()\n# And finally show the plot in a new window.\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"12c0c9133af2585e4f23ff93d0add32cd56967b9","_cell_guid":"bb5a44cd-fdd3-4ee7-9375-6169943174d0"},"cell_type":"markdown","source":"# * **We see that the data set has the housing price from 1996 to 2017. We can use this data to predict the price of 2018 and next years. ** *"},{"metadata":{"_uuid":"f5a948524d171a99fdaa1a5995e14e89922f132a","_cell_guid":"30186839-b64f-416e-92eb-3138d44734ba"},"cell_type":"markdown","source":"\n<a id=\"process_data\"></a>\n# ** 2. Pre-process the data **\n> ** In this section, I will prepare the dataset for feeding into LSTM network. Basically, I will do 4 things. **\n\n > 1.  ** Transform data to stationary ** \n > 2. ** Transform data to supervised learning **\n > 3. ** Split the data into train and test **\n > 4.  ** Scale the data to (-1,1) **"},{"metadata":{"_uuid":"7f6d831b1c0a515b312d0358fc14ec7283544dc3","_kg_hide-output":true,"_kg_hide-input":true,"_cell_guid":"7e2d5e7a-fbaf-40d2-9b96-0385aec04b96","trusted":true},"cell_type":"code","source":"# frame a sequence as a supervised learning problem\n# this methods will create a column and column value will be 1 shift from the data. \n# it will make our data to supervised so that we can feed into network\ndef timeseries_to_supervised(data, lag=1):\n\tdf = pd.DataFrame(data)\n\tcolumns = [df.shift(i) for i in range(1, lag+1)]\n\tcolumns.append(df)\n\tdf = pd.concat(columns, axis=1)\n\tdf.fillna(0, inplace=True)\n\treturn df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1a98efdfd4d2c10e3c1219e9dc78f66e6ebd1741","_kg_hide-output":true,"_kg_hide-input":true,"_cell_guid":"116ece99-3a53-400f-8b97-0eafb822fb96","trusted":true},"cell_type":"code","source":"# create a differenced series\ndef difference(dataset, interval=1):\n\tdiff = list()\n\tfor i in range(interval, len(dataset)):\n\t\tvalue = dataset[i] - dataset[i - interval]\n\t\tdiff.append(value)\n\treturn pd.Series(diff)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ba24123524b790dbb4c2840de2e57a656c23c1e","_kg_hide-output":true,"_kg_hide-input":true,"_cell_guid":"914d9380-e168-446c-b5d1-957a06682848","trusted":true},"cell_type":"code","source":"# invert differenced value\ndef inverse_difference(history, yhat, interval=1):\n\treturn yhat + history[-interval]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75c72b30fb80a71196fdc72739bbd48977a05f58","_kg_hide-output":true,"_kg_hide-input":true,"_cell_guid":"c752cd3c-04cd-4357-b9d7-4127016a50b7","trusted":true},"cell_type":"code","source":"# scale train and test data to [-1, 1]\ndef scale(train, test):\n\t# fit scaler\n\tscaler = MinMaxScaler(feature_range=(-1, 1))\n\tscaler = scaler.fit(train)\n\t# transform train\n\ttrain = train.reshape(train.shape[0], train.shape[1])\n\ttrain_scaled = scaler.transform(train)\n\t# transform test\n\ttest = test.reshape(test.shape[0], test.shape[1])\n\ttest_scaled = scaler.transform(test)\n\treturn scaler, train_scaled, test_scaled","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e02b69a461c2fa628f8d996072f6601d3961fa78","_kg_hide-output":true,"_kg_hide-input":true,"_cell_guid":"7acb2742-f0fc-451d-bdb8-e5e61d53bb06","trusted":true},"cell_type":"code","source":"# inverse scaling for a forecasted value\ndef invert_scale(scaler, X, value):\n\tnew_row = [x for x in X] + [value]\n\tarray = numpy.array(new_row)\n\tarray = array.reshape(1, len(array))\n\tinverted = scaler.inverse_transform(array)\n\treturn inverted[0, -1]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"318b5ba1fe99403e8f3dd0d11a30f915d167bf8b","_kg_hide-output":true,"_kg_hide-input":true,"_cell_guid":"4d8850c2-c1e3-47c9-9222-ed037e467033","trusted":true},"cell_type":"code","source":"# fit an LSTM network to training data\ndef fit_lstm(train, batch_size, nb_epoch, neurons):\n\tX, y = train[:, 0:-1], train[:, -1]\n\tX = X.reshape(X.shape[0], 1, X.shape[1])\n\tmodel = Sequential()\n\tmodel.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n\tmodel.add(Dense(1))\n\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n\tfor i in range(nb_epoch):\n\t\tmodel.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n\t\tmodel.reset_states()\n\treturn model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"40d47e1096abfcaafb14f66ec59ebb7c08effb7b","_kg_hide-output":true,"_kg_hide-input":true,"_cell_guid":"43f11a43-2340-4854-82c6-a086e0834595","trusted":true},"cell_type":"code","source":"# make a one-step forecast\ndef forecast_lstm(model, batch_size, X):\n\tX = X.reshape(1, 1, len(X))\n\tyhat = model.predict(X, batch_size=batch_size, verbose=0)\n\treturn yhat[0,0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2cf8a318f64a201eb8d9bcb2c8dff0014aca388e"},"cell_type":"code","source":"series = pd.Series(df_zhvi_sqft_all)\nraw_values = series.values\ndiff_values = difference(raw_values, 1)\nsupervised = timeseries_to_supervised(diff_values, 1)\nsupervised_values = supervised.values\ntrain_size = int(len(supervised_values) * 0.66)\ntrain_size","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da6c2036221bb04d8b511bd08d8a601adc163772","collapsed":true,"_cell_guid":"bd5a43fe-ec87-48f7-af2b-159001b2a8ad","trusted":false},"cell_type":"code","source":"# convert our column to pandas series \nseries = pd.Series(df_zhvi_sqft_all)\n# transform data to be stationary\nraw_values = series.values\ndiff_values = difference(raw_values, 1)\n\n# transform data to be supervised learning\nsupervised = timeseries_to_supervised(diff_values, 1)\nsupervised_values = supervised.values\n\n# split data into train and test-sets\ntrain_size = int(len(supervised_values) * 0.66)\ntrain, test = supervised_values[0:train_size], supervised_values[train_size:len(supervised_values)]\n\n# transform the scale of the data\nscaler, train_scaled, test_scaled = scale(train, test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ac88585a27cac3ff17098bdcee703f1a1e4d09f","_cell_guid":"04328e97-655a-40f5-8655-d46302b1cff0"},"cell_type":"markdown","source":"<a id=\"train_model\"></a>\n# ** 3. Train the model **\n> ** In this section, I will be training the model using data that we prepare in previous steps. **"},{"metadata":{"_uuid":"5815e3c11cb49cba3ca8380bacb7d4e7f747d9d7","scrolled":true,"_cell_guid":"fd227845-5ca3-47be-9f83-5d87091f9e66","trusted":false},"cell_type":"code","source":"# repeat 30 times\nrepeats = 30\n# variable for keep track of error scores\nerror_scores = list()\nfor r in range(repeats):\n    # let's train\n    lstm_model = fit_lstm(train_scaled, 1, 30, 4)\n    predictions = list()\n    # let's predict for test case\n    for i in range(len(test_scaled)):\n        # make one-step forecast\n        X, y = test_scaled[i, 0:-1], test_scaled[i, -1]\n        yhat = forecast_lstm(lstm_model, 1, X)\n        # invert scaling\n        yhat = invert_scale(scaler, X, yhat)\n        # invert differencing\n        yhat = inverse_difference(raw_values, yhat, len(test_scaled)+1-i)\n        # store forecast\n        predictions.append(yhat)\n        # report performance\n    rmse = sqrt(mean_squared_error(raw_values[train_size:len(supervised_values)], predictions))\n    print('%d) Test RMSE: %.3f' % (r+1, rmse))\n    error_scores.append(rmse)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0cb8429b72e888e169ff41ab9b7d7be4ad3bede3","_kg_hide-output":true,"collapsed":true,"_kg_hide-input":true,"_cell_guid":"67bd1ff1-c6f3-421c-86b9-601fd5f12da1","trusted":false},"cell_type":"code","source":"# report performance\n# summarize results\nresults = pd.DataFrame()\nresults['rmse'] = error_scores\n# print(results.describe())\n# results.boxplot()\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59ca74f03eb2df2b989e6c9458752bfc55d2370a","_cell_guid":"92bf9d26-c680-46c8-81a7-02ff67297213","trusted":false},"cell_type":"code","source":"# line plot of observed vs predicted\nplt.figure(figsize=(15, 10))\nplt.plot(raw_values[train_size:len(supervised_values)])\nplt.plot(predictions)\nplt.title('Original data vs predicted data', fontsize=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4df0815ea26e430511619ca3fbbfaf8520825f6e","_kg_hide-output":true,"collapsed":true,"_kg_hide-input":true,"_cell_guid":"0032aa1f-da62-420c-8f58-aaafc11adc0d","trusted":false},"cell_type":"code","source":"# last value of our dataset\nf = np.array(151.072060)\n# sklearn minmaxscaler for converting \"f\" to range to (-1,1)\nscaler = MinMaxScaler(feature_range=(-1, 1))\n# minmaxscaler fit\nscaler = scaler.fit(f)\n# let's transform\ntrain = f.reshape(1, 1)\ntrain_scaled = scaler.transform(train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f2f673107ab78bd7495896e847271240d32693ca","_cell_guid":"d9101a37-bd7f-4aa8-8b70-8d807aa0f45b"},"cell_type":"markdown","source":"<a id=\"predict_house_price\"></a>\n# ** 4. Predict the house price for next years **\n> ** In this section, I will be prediction the housing price for next years given the the last day price value of data set. And plot the prediction in graph** "},{"metadata":{"_uuid":"604fce7773aa0b9ab00be7766d3c55ec58f76fd9","collapsed":true,"_cell_guid":"53b5a9fe-c38e-43b8-8f68-77c2aeee60c6","trusted":false},"cell_type":"code","source":"# initial record\ninitial = train_scaled\n# store prediction\nprediction = []\n# range 40 because we want the prediction for next 40 months\nfor i in range(40):\n    # predict \n    yhat = forecast_lstm(lstm_model, 1, initial)\n    # inverse prediction to it's original value\n    yhat_inver = scaler.inverse_transform(yhat)\n    # append to our prediction variable\n    prediction.append(yhat_inver)\n    # re initial our initial variable so that it feed the current predicted value as input for forecast\n    initial = np.array([yhat])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1520e92deadef7526b3b019cf760bd63c5fdbcc5","_kg_hide-output":true,"collapsed":true,"_kg_hide-input":true,"_cell_guid":"e4700fc3-9cca-43fc-be56-d55486850ad8","trusted":false},"cell_type":"code","source":"prediction = np.concatenate(prediction, axis=0 ).tolist()\nprediction = [item for sublist in prediction for item in sublist]\nprediction = pd.DataFrame(prediction)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c5957bdbfebb74b25d7957d7b2d4429bbe66f563","_kg_hide-input":true,"_cell_guid":"30c1ba60-63cc-46c0-908e-7874eacd9281","trusted":false},"cell_type":"code","source":"rng = pd.date_range('2017-08-31 00:00:00', periods=40, freq='M')\nrng = pd.DataFrame(rng)\nprediction = pd.merge(rng, prediction, left_index=True, right_index=True, how='outer')\nprediction.set_index('0_x')\nprediction.columns = ['Date', 'ZHVIPerSqft_AllHomes']\nprediction","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5b352a3b2633711731bab45c9256fbfed2912f59","_kg_hide-output":true,"_kg_hide-input":true,"_cell_guid":"e9595483-315b-4b88-ae26-3d667c1a365e","trusted":false},"cell_type":"code","source":"original = pd.DataFrame({'Date':df_zhvi_sqft_all.index, 'ZHVIPerSqft_AllHomes':df_zhvi_sqft_all.values})\noriginal","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5fc0bff110443dcf78582daef1a13c81a39de119","_kg_hide-output":true,"collapsed":true,"_kg_hide-input":true,"_cell_guid":"acbd2c8b-6919-4834-bb58-70f1c99a576e","trusted":false},"cell_type":"code","source":"frames = [original, prediction]\ndf_final = pd.concat(frames)\ndf_final.set_index(['Date'],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b20251f975241db503e5080a715f52a9e9a42634","_kg_hide-output":true,"collapsed":true,"_kg_hide-input":true,"_cell_guid":"cf8f8a4d-9b7b-4508-8da7-2979125b9866","trusted":false},"cell_type":"code","source":"df_final = pd.Series(df_final.ZHVIPerSqft_AllHomes)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"54fa2f127dbed481aa957020fc89fff710a3063e","_kg_hide-output":true,"collapsed":true,"_kg_hide-input":true,"_cell_guid":"1cf126fa-7522-4d9e-a2f1-edc189960f30","trusted":false},"cell_type":"code","source":"plt.rc('figure', figsize=(15, 10))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d57c5fcf0cffed8ccdf600f82b152139180a5891","_kg_hide-output":false,"_kg_hide-input":false,"_cell_guid":"913d9c92-e96f-4d7c-96f9-ca31d7d76a21","trusted":false},"cell_type":"code","source":"# df_final.plot(figsize=(15, 5),x_compat=True)\nfig, ax = plt.subplots(figsize=(15, 10))\nax.scatter(df_final.index, df_final)\nax.xaxis.set_major_locator(mdates.YearLocator(1))\n# ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\nplt.title('Zillow Home Value Index in different year', fontsize=18)\nplt.xlabel('Year', fontsize=18)\nplt.ylabel('Zillow Home Value Index in per Square foot', fontsize=18)\nfig.autofmt_xdate()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"46e0b04f6abf857b5927ede3513eaf80720b414e","collapsed":true,"_cell_guid":"202f04d1-c57f-4cd4-8b42-4bfca39e8fd4"},"cell_type":"markdown","source":"# ** 5. Decision **\n> ** It seems like in 2018 - 2021, the housing price will not increase or decrease so much. In 2018 it will change a very little bit down.**\n>> ** How perfect the result is? Well I don't say it's the correct prediction. But gives us an idea. **\n\n>  ** I am new the Data science and machine learning field. If you have found any mistakes and have suggestion feel free to comment. Thank you very much! **"},{"metadata":{"_uuid":"aa53a76c334855364e7e81d99474d45d66ab2c3d","collapsed":true,"_cell_guid":"bfa36ed3-bbc7-4df1-831e-27d724fa6878","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}